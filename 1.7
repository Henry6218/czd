# 大模型推理系统基础知识总结

# 大模型推理系统的组成部分
# 1. 用户请求（应用、UI）
# 2. 处理用户请求（多个用户请求，对话框、API 等等）
# 3. 大模型、神经网络
# 4. 算子、参数、资源管理
# 5. 加速硬件
# 需在特定平台上，结合不同加速卡、操作系统进行针对性优化

# 大模型定义
# 具有大规模参数和复杂计算结构的机器学习模型，由深度人工神经网络构成

# 传统模型的局限性
# 传统模型使用已知数学公式（如多项式），参数未知但公式已知
# 目标是找到一组最优参数集合，使公式以最优方式拟合数据点
# 若找不到相应参数，则尽可能逼近数据
# 弊端：需要进行一些假设，若数学公式不符合实际，无法找到合适参数

# 人工神经网络
# 由大量神经元构成，每个神经元包含线性计算和非线性计算
# 线性计算改变输入信号的权重（放大或缩小）和偏移
# 非线性计算代表神经元是否激活

# 神经网络决策
# 参数：w 权重；b 偏移
# 简单决策公式：w0x0 + w1x1 + b = 0
# 复杂情况可引入两次判断（使用两个神经元）
# 无论多复杂的多边形，都可通过画多根直线拟合

# 万能近似定理
# 三层神经网络（一个输入层，一个隐藏层，一个输出层）
# 在理论上可以近似任意复杂（有限维度）的决策边界
# 若输入维度更高，增加 x 的数量；若输出种类更多，增加 y 的数量

# 例子：图像分类
# 每张图可看作长度 28 * 28 的向量
# 神经网络进行“画线操作”后输出，为数字图片分类

# 训练与推理
# 训练：通过大量已知数据调整模型参数，使其更准确地对新数据进行预测
# 步骤：定义损失函数，按减少损失的方向调整参数（梯度下降法），要有好的模型结构
# 推理：使用训练好的参数，对新的数据进行预测
# 三层神经网络有时难找到特征关系，或数字向量部分元素为空
# 需从高维数据抽象出特征，引导训练过程

# 深度神经网络优势
# 通过更多层次的解构，用较少参数捕捉更复杂的特征关系，提升参数效率
# 特殊结构如 CNN、RNN、Transformer，可引导模型用更少参数捕捉特定特征

# 大模型核心算子
# 矩阵乘：
# - 批量进行神经元的线性计算 y = xw^T + b
# - 大模型计算中总耗时最长的算子
# - 存储权重矩阵时要进行转置操作

# 大模型算子：激活函数
# 属于非线性计算，训练时需对函数求导才能进行梯度下降

# 大模型中的神经网络：多层感知器（MLP），也称为前馈神经网络（FFN）
# 性质：
# - 推理时参数固定，同样的输入意味着同样的输出
# - 多个输入向量之间不会互相影响，没有顺序之分
# 此方式无法实现类似 ChatGPT 的大语言模型，后续可用 Transformer 结构解决

# 大模型的规模
# 1B = 十亿（10^9）参数 * （数据类型大小）
# - fp16、bf16 是最常见的数据类型，更小的量化类型如 int8 成为趋势
# - 每个权重矩阵可能有上亿个参数
# - 一个模型可有几十甚至上百层神经网络
# - 神经网络的激活很稀疏，许多权重计算结果为 0，带来优化空间

# 模型文件格式相关知识

# 纯二进制文件格式
# 如 .bin/.pt/.ckpt 等，基本不具备人类可读性，脱离原程序就无法使用

# safetensors 格式（由 hugging face 提出，目前最广泛）
# config.json：存储模型配置常数，如模型类型、词表大小、层数等
# model.safetensors：存储模型参数，可分多个文件存储，由一个 json 文件记录信息
# tokenizer.model：存储词表与 tokenizer 信息，tokenizer 还有额外的 json 文件

# gguf 格式（由 Ilama.cpp 提出）
# 所有模型信息和参数都存在同一个文件中

# 各层中间结果
# 输入和输出形状相同，均为 (L, hidden_size)
# 输出矩阵形状为 (hidden_size * vocab_size)

# 文本生成——因果语言模型(Causal LM)
# 对于每个输入 token，模型输出下一个 token 的概率分布
# 例如，“中” 之后可能是 “国”、“间” 等，每个词都有对应的概率
# 根据概率分布进行分析，不断循环便能实现文本生成

# 采样方法
# Argmax：选取概率最大的词
# 缺点：生成结果单一，缺乏创意，可能导致同样的句子循环

# 随机采样：按概率随机选取
# 缺点：有一定概率选到错误的词

# Top - k：从概率最大的 k 个词中随机抽样
# Top - p：按概率排序，从直到概率总合为 p 位置的前 n 个词中随机抽样
# Temperature 越大，模型越 “有创意”；反之则越 “自信”，取值有区间

# Beam Search
# 保留总概率最大的 n 个分支
# 优点：效果最好
# 缺点：保留分支会占用大量资源，一般网络服务不会使用

# 多轮推理
# 如果每次增加输入长度，推理性能会越来越差，存在重复计算问题

# KV Cache
# 计算注意力分数时，都是拿 Q 与 K 计算，再乘上 V
# 每一轮用到对应的 Q、K、V
# 观察发现，只需要额外算新输入的 Q
# 存储前面轮次的 K、V，每次只需要新算一个小格
# use_cache: true（是否使用 kv cache）
# max_position_embeddings: 4096（最长序列长度）
# 使用 KV Cache 计算量依旧增加，但比反复计算 K、V 少很多

# Prefill 和 Decode
# 每次输入一个 Token，输出一个 token，可以拆分成两个阶段

# AI 对话
# 1. 续写故事
# 输入："There was a"
# 输出：“girl named lily, who lived in..."

# 2. 对话生成是带着身份信息的续写故事
# 需要一个 Template，如 "<user>Hey there!<Al>Nice to meet you!"
# 为用户输入的开头/结尾加上特定的 tag，形式由 tokenizer 和模型决定

# AI 对话 Template
# Jinja2：Python 模板引擎（hugging face 统一使用）
# 模板信息存储于 tokenizer_config.json 中，如 system、user、assistant
# 要确定使用的是正确的 Template

# 同时服务大量用户并降低平均延迟的方法总结

# 模型层优化目标
# - 提升推理速度
# - 降低存储需求
# - 提升输出质量

# 系统层优化目标
# 最大化利用硬件算力、存储、通信资源

# UI：推理服务中的用户请求类型
# 1. 创建/切换/删除会话
# 2. 修改历史对话重推理
# 3. 切换历史对话
# 4. 推理/中断推理

# 多用户推理服务问题及解决方案

# 多用户AI对话服务存在的问题
# 每个用户的请求：
# 1. 输入长度不同，需要padding，会造成计算浪费
# 2. 输出长度不同，需要等待最慢的请求，导致延迟增加
# 对于像CNN等静态模型，batching会降低服务效率

# 连续批次策略 - Orca
# 以迭代为单位的Batching
# 每轮从请求池中选出若干输入组成批次，推理结束的请求离开，新请求加入
# 可以进行batching的计算同时进行，无法batching的计算分请求进行(attention score)
# 优点：
# - 提升硬件利用率
# - 提升系统吞吐量
# - 降低平均延迟

# 混合精度优化
# 不同精度的浮点数类型：
# FP32：有时存在浪费
# FP16：很常用，范围和精度比FP32小，但10个小数位通常够用，对推理质量影响小
# BF16：常用，最大值最小值和FP32相同，只有7位小数位，精度比FP32低，可解决FP16范围不够的问题
# TF32：有FP32的表示范围，有FP16的精度，但不是所有硬件都支持

# 特殊情况：当指数位全1
# - 如果fraction全0，则表示+inf或者-inf
# - 如果fraction不全为0，则表示NaN

# 由于神经元只需要得出是否激活的结论，对精度要求不高

# 量化推理
# 将浮点数映射到数据更小的int8/int4类型上
# 有些芯片支持，有些芯片不支持，需要有反量化的过程，有的只有参数存成int8
# 存在许多不同的量化策略

# 模型结构优化

# 减少K/V Cache大小
# 原始K/V Cache大小：2 * nlayer * max_len * n_head * head_dim
# 优化方法：从MHA到MQA、GQA
# 多组Q对应一组KV从而减少KV的总大小
# 优化后K/V Cache大小：2 * n_layer * max_len * n_kv_head * head_dim
# 其中n_head变成n_kv_head

# 减少大模型的计算
# 稀疏性：神经网络中少量神经元对输出有较大影响，大部分神经元影响很小甚至没有激活
# 混合专家(MoE)模型
# 大模型分布式推理相关知识点总结

# 不同层次的优化策略
# 服务层：采用分流、请求池、连续批次等策略
# 推理系统层：运用分布式推理、内存换入换出、投机采样等技术
# 模型层：使用量化、MoE、图优化等方法
# 计算层：采用FlashAttention等融合算子、代码生成等手段

# 资源权衡
# 在资源一定的情况下，存在计算延迟、内存占用和精度损失之间的权衡

# 背景信息
# 1B（十亿，10^9）参数使用FP16约占2GB内存
# 常见英伟达GPU内存大小：
# NV H200：141G
# NV A100：80G
# NV L40：48G
# NV V100：32G

# 分布式推理策略
# 1. 数据并行
# 每台机器上都有完整参数，可以提高吞吐量，但不能减少空间占用
# 2. 模型并行
# 每台机器上有部分参数
# 3. 流水线并行
# 将模型前后切分，每个参数是完整的（前一半在一张卡上，后一半在另一张卡）
# 简单，可提升吞吐量，但不能降低延迟（需数据源源不断进来才能体现效果，且包含通信开销）
# 4. 张量并行
# 每个参数都分布在不同卡上，可以降低延迟，但较为复杂，需要同步

# 张量切分状态
# 完整(replication)：每个硬件都有相同的完整张量
# 切分(partition)：张量按照某个维度切分，每个硬件持有一片，一般分为行切分和列切分
# 部分和(partial sum)：每个硬件持有一个和原张量形状一样的张量，完整张量 = 所有硬件持有张量之和

# 同步操作：将处于切分或部分和状态的张量还原成完整张量

# 通信算子
# AllGather：同步切分张量
# AllReduce(Sum)：同步部分和张量

# NCCL通信库使用方式
# 单进程单线程多卡：直接创建四个通信器，在需要通信时用groupStart()和groupEnd()进行allReduce操作
# 每张卡一线程/进程

# 分布式矩阵乘
# 分布式矩阵乘1：左边为完整，右边为列切分，结果为列切分，用AllGather同步
# 分布式矩阵乘2：左边为列切分，右边为行切分，结果为部分，用AllReduce同步

# 大模型分布式推理各层分析

# MLP层
# X(s, d) -> (s, i/n)（此时不需要进行同步） -> (s, d)部分和 -> AllReduce -> Y(s, d)
# 根据intermediate_size维度进行切分，相较于第一种少了两次同步
# 总结：
# - 所有参数按照intermediate_size维度切分
# - 结尾处用AllReduce同步
# - 注意残差的处理

# Attention层
# 每个head的计算是独立的，按照头的维度切分，多个硬件时每个计算一个/多个头
# 所有计算完后，做一次AllReduce进行同步
# 总结：
# - 所有参数按照n_head维度切分
# - 注意切分后需要进行必要的转置
# - Q的head数和KV的head数可能不同
# - 结尾处用AllReduce同步
# - 注意残差的处理

# RMS Norm层
# 不进行分布式计算（需要进行所有数字求和）

# Input Embedding层
# 不进行分布式计算

# Output Embedding层
# 可以按单个分布式矩阵乘计算，用AllGather同步
# 推理时每轮只吐一个token所以不是很有必要
